{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81b0ebf-937a-4457-b608-19aa80adf282",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:56.277062Z",
     "iopub.status.busy": "2023-10-15T04:06:56.276396Z",
     "iopub.status.idle": "2023-10-15T04:06:56.286303Z",
     "shell.execute_reply": "2023-10-15T04:06:56.282392Z",
     "shell.execute_reply.started": "2023-10-15T04:06:56.276998Z"
    }
   },
   "outputs": [],
   "source": [
    "# for notbook shortcut command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca2edb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:56.291011Z",
     "iopub.status.busy": "2023-10-15T04:06:56.290014Z",
     "iopub.status.idle": "2023-10-15T04:06:58.119483Z",
     "shell.execute_reply": "2023-10-15T04:06:58.118182Z",
     "shell.execute_reply.started": "2023-10-15T04:06:56.290960Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os, pdb, warnings, torch, time\n",
    "sys.path.insert(0, './core/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from minilib import *\n",
    "from utmLib import utils\n",
    "from utmLib.clses import Logger\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.rcParams[\"figure.figsize\"] = [10,2]\n",
    "\n",
    "# this is mainly for papermill parameter detection\n",
    "try:\n",
    "    dummy_x89757 = data_name\n",
    "except:\n",
    "    data_name = 'parkinson'\n",
    "    log_file = 'results/exp.log'\n",
    "    G_delta = 0.5\n",
    "    testing = 1\n",
    "\n",
    "logger = Logger(log_file)\n",
    "res_log = Logger(log_file + '.res', with_time = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f35cd-de6f-498d-a7f3-156310f92f92",
   "metadata": {},
   "source": [
    "# Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4bfd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.120623Z",
     "iopub.status.busy": "2023-10-15T04:06:58.120302Z",
     "iopub.status.idle": "2023-10-15T04:06:58.128661Z",
     "shell.execute_reply": "2023-10-15T04:06:58.128039Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.120602Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_imgs(img_array, rows = 2, cols = 8, selected = None, label_array = None):\n",
    "    if img_array[0].size == 196:\n",
    "        process = lambda x:x.reshape(14,14)\n",
    "    else:\n",
    "        process = lambda x:x.reshape(28,28)\n",
    "    \n",
    "    # random select some examples for display if not specified\n",
    "    if selected is None:\n",
    "        assert(img_array.shape[0] >= rows * cols)\n",
    "        selected = np.random.choice(img_array.shape[0], rows * cols, replace = False)\n",
    "    else:\n",
    "        assert(selected.size >= rows * cols)\n",
    "    \n",
    "    k = 1\n",
    "    labels = []\n",
    "    fid = plt.figure()\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            plt.subplot(rows, cols, k)\n",
    "            plt.imshow(process(img_array[selected[k-1]]), cmap='gray' )\n",
    "            if label_array is not None:\n",
    "                labels.append(label_array[selected[k-1]])\n",
    "            k += 1\n",
    "            plt.axis('off')\n",
    "    \n",
    "    if len(labels):\n",
    "        print(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b70dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.130161Z",
     "iopub.status.busy": "2023-10-15T04:06:58.129877Z",
     "iopub.status.idle": "2023-10-15T04:06:58.216471Z",
     "shell.execute_reply": "2023-10-15T04:06:58.214931Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.130144Z"
    }
   },
   "outputs": [],
   "source": [
    "def adversial_step(masses, delta = 0.5):\n",
    "    \n",
    "    def get_w(alpha):\n",
    "        w = np.exp( -masses / alpha )\n",
    "        w = w / np.sum(w) * N\n",
    "        return w\n",
    "    \n",
    "    def valid(alpha):\n",
    "        w = get_w(alpha)\n",
    "        return np.log(np.power(w,w)).sum() <= M\n",
    "    \n",
    "    # use the idea of binary search, time complexity O(N * lgN)\n",
    "    N = masses.size\n",
    "    M = N * delta\n",
    "    l = 0.01\n",
    "    r = 2 ** 10\n",
    "    \n",
    "    # need to gurantee that r is big enough \n",
    "    while not valid(r):\n",
    "        r = r * 2\n",
    "    \n",
    "    # binary search a valid alpha in range [l,r] \n",
    "    while r - l > 1e-2:\n",
    "        m = (l+r) / 2\n",
    "\n",
    "        if valid(m):\n",
    "            r = m \n",
    "        else:\n",
    "            l = m\n",
    "            \n",
    "    return get_w(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f98823a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.221247Z",
     "iopub.status.busy": "2023-10-15T04:06:58.220700Z",
     "iopub.status.idle": "2023-10-15T04:06:58.335251Z",
     "shell.execute_reply": "2023-10-15T04:06:58.333864Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.221197Z"
    }
   },
   "outputs": [],
   "source": [
    "def py_learning_step(step, R, data_loader, model, optimizer, scheduler, max_epoch):\n",
    "    model.train()\n",
    "    R = R.astype('f4').reshape(-1, 1)\n",
    "    R_tensor = torch.from_numpy( R ).to(model.device)\n",
    "    \n",
    "    for e in range(max_epoch):\n",
    "        acc_loss = 0\n",
    "        total = 0\n",
    "\n",
    "        for X, Y, _i in data_loader:\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(X)\n",
    "            loss = model.loss(out, Y, R = R_tensor[_i])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss += loss.item()\n",
    "            total += X.shape[0]\n",
    "        \n",
    "        # step is global variable\n",
    "        scheduler.step()\n",
    "        if VERBOSE >= 1:\n",
    "            print(f'Step {step}, Learning epoch {e}, avg loss: {acc_loss/total}', end = '\\r')\n",
    "    return acc_loss/total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1e5925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.337698Z",
     "iopub.status.busy": "2023-10-15T04:06:58.337154Z",
     "iopub.status.idle": "2023-10-15T04:06:58.417359Z",
     "shell.execute_reply": "2023-10-15T04:06:58.415747Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.337648Z"
    }
   },
   "outputs": [],
   "source": [
    "def mg_learning_step(R, data, **kwargs):\n",
    "    # learn a gaussian distribution from weighted data\n",
    "    N, D = data.shape \n",
    "    mg = MultivariateGaussain()\n",
    "    R = R.reshape(N, 1)\n",
    "    mg.mu = np.mean( R * data, axis = 0)\n",
    "    \n",
    "    mat = data - mg.mu.reshape(1, D)\n",
    "    mat2 = mat.copy()\n",
    "    mat2 = mat2 * R / N\n",
    "    S2 = mat.T @ mat2\n",
    "    \n",
    "    mg.S = S2\n",
    "    return mg\n",
    "\n",
    "def mixmg_learning_step(R, data, n_comp, **kwargs):\n",
    "    step = kwargs['step']\n",
    "    total = kwargs['total']\n",
    "    adv = kwargs['adv']\n",
    "    wrapper = kwargs['wrapper']\n",
    "    \n",
    "    if wrapper[0] is None:\n",
    "        wrapper[0] = MixMGLearner(max_iter = 0, n_components = n_comp,\n",
    "                                  reg_covar = 1e-4).fit(data)\n",
    "    \n",
    "    mixmg_object = wrapper[0]\n",
    "    \n",
    "    if step == 0:\n",
    "        niter = 50\n",
    "    else:\n",
    "        niter = 1\n",
    "        \n",
    "    for _ in range(niter):\n",
    "        mixmg_object._estep()\n",
    "        mixmg_object._mstep(data, R)\n",
    "    \n",
    "    return mixmg_object.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fc73ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.419882Z",
     "iopub.status.busy": "2023-10-15T04:06:58.419324Z",
     "iopub.status.idle": "2023-10-15T04:06:58.544331Z",
     "shell.execute_reply": "2023-10-15T04:06:58.542704Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.419832Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, data, _x, _y):\n",
    "        self.X = data[:, _x]\n",
    "        self.Y = data[:, _y]\n",
    "        self.total = data.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.X[ind], self.Y[ind], ind\n",
    "\n",
    "def five_number_statistic(logmass):\n",
    "    p25, median, p75 = np.percentile(logmass, [25,50,75])\n",
    "    average = np.mean(logmass)\n",
    "    std = np.std(logmass)\n",
    "    ret = (p25, median, p75, average, std)\n",
    "    return list(np.round(ret, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc63103-9269-4d72-a65f-d1cc8b0f4b22",
   "metadata": {},
   "source": [
    "# training and experiment helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cfdb7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.547271Z",
     "iopub.status.busy": "2023-10-15T04:06:58.546535Z",
     "iopub.status.idle": "2023-10-15T04:06:58.630488Z",
     "shell.execute_reply": "2023-10-15T04:06:58.628896Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.547218Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, train_conf, model_conf, lr, wd, device):\n",
    "    model_conf.device = device\n",
    "    xid = model_conf.xid\n",
    "    yid = model_conf.yid\n",
    "    num_parents = model_conf.num_parents\n",
    "    structure = model_conf.structure\n",
    "    \n",
    "    batch_size = train_conf.batch_size\n",
    "    init_epoch = train_conf.init_epoch\n",
    "    n_epoch = train_conf.n_epoch\n",
    "    n_step = train_conf.n_step\n",
    "    adv = train_conf.adversial\n",
    "    delta = train_conf.delta\n",
    "    n_pretrain = train_conf.n_pretrain\n",
    "    pre_epochs = train_conf.pre_epochs\n",
    "\n",
    "    if VERBOSE >= 2:\n",
    "        model_ids = [id(m) for m in train_conf.model_objs]\n",
    "        print('Model IDs: {}'.format(model_ids) )\n",
    "    \n",
    "    # init\n",
    "    px_container = [None]  # for mixmg px part use only in concurrent environment\n",
    "    R = np.ones( shape = (train_data.shape[0], ) )\n",
    "    total_epochs = init_epoch + n_epoch + n_step - 1 \n",
    "    data_loader = DataLoader( MyData(train_data, xid, yid) , batch_size = batch_size)\n",
    "\n",
    "    # pretraining, reduce the effect of random init of neural network \n",
    "    best_score = np.inf\n",
    "    best_nn = None\n",
    "    for i in range(n_pretrain):\n",
    "        # model = PGNN( len(xid) , num_parents, model_conf)\n",
    "        model = deepcopy(train_conf.model_objs[i])\n",
    "        model.moveto(device)\n",
    "        model.loss = partial(model.weighted_loss_func, G = structure)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), weight_decay = wd )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr,\n",
    "                                anneal_strategy='cos', pct_start=0.2,\n",
    "                                epochs= total_epochs, steps_per_epoch = 1, verbose = False)\n",
    "        model.opt = optimizer\n",
    "        model.sch = scheduler\n",
    "        score = py_learning_step('Init', R, data_loader, model, optimizer, \n",
    "                     scheduler, max_epoch = pre_epochs)\n",
    "\n",
    "        if VERBOSE >= 2:\n",
    "            print(f'In trial #{i}, with loss {score}')\n",
    "        \n",
    "        if score < best_score:\n",
    "            if VERBOSE >= 2:\n",
    "                print(f'update the best model as trial #{i}')\n",
    "            best_nn = model\n",
    "            best_score = score\n",
    "\n",
    "    # revert to the best model \n",
    "    model = best_nn\n",
    "    optimizer = model.opt\n",
    "    scheduler = model.sch\n",
    "\n",
    "    # finish train to init_epoches \n",
    "    py_learning_step('Init', R, data_loader, model, optimizer, \n",
    "                     scheduler, max_epoch = max(init_epoch - pre_epochs, 0) )\n",
    "    \n",
    "    # create the p(y|x) model object\n",
    "    py = NeuralNetCondMG()\n",
    "    py.nn = model\n",
    "    py.gbn = GBN(structure).fit(train_data[:, yid], var_thresh = 1e-2)\n",
    "    \n",
    "    # begining of (adversial robust) training \n",
    "    best_score = -np.inf\n",
    "    for step in range(n_step):\n",
    "        # conduct learning step first\n",
    "        model.train()\n",
    "        mg = px_learning_step(R, train_data[:, xid], step = step, total = n_step,\n",
    "                              adv = adv, wrapper = px_container)\n",
    "        py_learning_step(step, R, data_loader, model, optimizer, \n",
    "                         scheduler, max_epoch = n_epoch)\n",
    "        model.eval()\n",
    "\n",
    "        # evaluate the mass of each data\n",
    "        cnet = ContCNet(mg, py, xid, yid)\n",
    "        masses = cnet.mass(train_data, logmode = True)\n",
    "\n",
    "        # conduct adversial step\n",
    "        if adv:\n",
    "            R = adversial_step(masses, delta = delta)\n",
    "\n",
    "        # 07-10-23 22:32, now gurantees to satisfy\n",
    "        # verify if R satisfy the constrains \n",
    "        # sat1 = np.sum(R) <= ( train_data.shape[0] + EPS )\n",
    "        # sat2 = np.sum( np.log( np.power(R, R) ) ) <= ( train_data.shape[0] * delta + EPS )\n",
    "        \n",
    "        # if not (sat1 and sat2):\n",
    "        #     warnings.warn(f'Constrains not satisfied during adversial step.')\n",
    "        #     break \n",
    "        \n",
    "        score = cnet.mass(val_data, logmode = True).mean()\n",
    "        if score > best_score:\n",
    "            best_model = deepcopy(cnet)\n",
    "            best_score = score \n",
    "    \n",
    "    # revert to best cnet \n",
    "    model = best_model.py.nn\n",
    "    model.eval()\n",
    "    model.moveto('cpu')\n",
    "    \n",
    "    return best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf826de-88c4-4ac6-a335-f720d3cbad3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.632974Z",
     "iopub.status.busy": "2023-10-15T04:06:58.632423Z",
     "iopub.status.idle": "2023-10-15T04:06:58.761292Z",
     "shell.execute_reply": "2023-10-15T04:06:58.759907Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.632924Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_neighbor_ll(model, data, seed = 7, num_nb = 500, eps = 0.1):\n",
    "    '''\n",
    "    compute the worst and average ll for each sample in the data \n",
    "    '''\n",
    "    N, D = data.shape \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    results = [] \n",
    "    for x in data:\n",
    "        pertubation = np.random.rand(num_nb, D)\n",
    "        pertubation = (pertubation - 0.5) * 2 * eps\n",
    "        nb_data = x.reshape(1,-1) + pertubation\n",
    "        nb_ll = model.mass(nb_data, logmode = True)\n",
    "\n",
    "        # we only care about the worst and average \n",
    "        results.append([np.min(nb_ll), np.mean(nb_ll)])\n",
    "\n",
    "    min_nb_ll, avg_nb_ll = list(zip(*results))\n",
    "    return min_nb_ll, avg_nb_ll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e695bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.764001Z",
     "iopub.status.busy": "2023-10-15T04:06:58.763448Z",
     "iopub.status.idle": "2023-10-15T04:06:58.882811Z",
     "shell.execute_reply": "2023-10-15T04:06:58.881433Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.763952Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import random \n",
    "\n",
    "\n",
    "def run_exp(dataset, train_conf, nn_conf, param = None):\n",
    "    # fix all random seed \n",
    "    my_seed = 7\n",
    "    torch.manual_seed(my_seed)\n",
    "    random.seed(my_seed)\n",
    "    np.random.seed(my_seed)\n",
    "    torch.cuda.manual_seed_all(my_seed)\n",
    "\n",
    "    if param is not None:\n",
    "        warnings.warn('Running with fixed hyper-parameter, you should do this only when testing.')\n",
    "\n",
    "    train_conf.show(ignore = {'model_objs'})\n",
    "    nn_conf.show()\n",
    "                \n",
    "    train_data = dataset.train\n",
    "    val_data = dataset.valid\n",
    "    \n",
    "    xid = nn_conf.xid\n",
    "    yid = nn_conf.yid\n",
    "    model_conf = deepcopy(nn_conf)\n",
    "    structure = create_graph(train_data[:, yid], \n",
    "                    max_parents = int(PARENT_RATIO * train_data.shape[1]), \n",
    "                    corr_thresh = CORR_THRESH)\n",
    "\n",
    "    model_conf.num_parents = [len(structure.V[i].parents) for i in range(structure.N)]\n",
    "    model_conf.structure = structure\n",
    "    model_conf.device = 'cpu'\n",
    "\n",
    "    if train_conf.model_objs is None:\n",
    "        # shared model objects across adversial and non-advesial \n",
    "        train_conf.model_objs = [ PGNN(len(xid), model_conf.num_parents, model_conf)\n",
    "            for i in range(train_conf.n_pretrain)]\n",
    "\n",
    "    # do hyper selection if param is not specified\n",
    "    \n",
    "    if param is None:\n",
    "        if testing:\n",
    "            learning_rates = [5e-3, 1e-2]\n",
    "            weight_decays = [1e-4]\n",
    "        else:\n",
    "            learning_rates = [1e-2, 3.3e-3, 1e-3]\n",
    "            weight_decays = [1e-3, 1e-4]\n",
    "\n",
    "        all_params = list(product(learning_rates, weight_decays))\n",
    "\n",
    "        results = Parallel(n_jobs = 1, prefer = 'threads')(\n",
    "            delayed(train_model)(train_data, val_data, train_conf, model_conf,\n",
    "                    *comb, cuda_device)\n",
    "            for comb in all_params\n",
    "        )\n",
    "\n",
    "        models, scores = list(zip(*results))\n",
    "        ind = np.argmax(scores)\n",
    "        cnet = models[ind]\n",
    "        print('Best hyper parameter is: {}'.format(all_params[ind]))\n",
    "    else:\n",
    "        # print('Running with manual hyper parameters: {}'.format(param))\n",
    "        cnet = train_model(train_data, val_data, train_conf, model_conf, *param, 'cuda:0')[0]\n",
    "\n",
    "\n",
    "    avg_stats = []\n",
    "    for test_X in dataset.test:\n",
    "        cur_mass = cnet.mass(test_X, logmode = True)\n",
    "        mass_stat = five_number_statistic(cur_mass)\n",
    "        logger.write('TestLL p25:{} median:{} p75:{} Mean:{} Std:{}'.format(*mass_stat))\n",
    "        avg_stats.append( str(mass_stat[3]) )\n",
    "\n",
    "    for cur_mass, name in zip(compute_neighbor_ll(cnet, dataset.test[0]),\n",
    "                            ['Worst NB LL','Avg NB LL']):\n",
    "                                \n",
    "        mass_stat = five_number_statistic(cur_mass)\n",
    "        logger.write('{} p25:{} median:{} p75:{} Mean:{} Std:{}'.format(name, *mass_stat))\n",
    "        avg_stats.append( str(mass_stat[3]) )\n",
    "\n",
    "    \n",
    "    res_log.write(','.join(avg_stats), echo = 0)\n",
    "    return cnet\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4a491",
   "metadata": {},
   "source": [
    "# Experiement of Loglikelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee44f3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:58.885343Z",
     "iopub.status.busy": "2023-10-15T04:06:58.884791Z",
     "iopub.status.idle": "2023-10-15T04:06:59.060777Z",
     "shell.execute_reply": "2023-10-15T04:06:59.059837Z",
     "shell.execute_reply.started": "2023-10-15T04:06:58.885293Z"
    }
   },
   "outputs": [],
   "source": [
    "# define some common parameters across experiments\n",
    "from models.ours.ContCNet import ContCNet\n",
    "from utmLib.ml.GBN import GBN\n",
    "from models.ours.NNCondMG import MyObject, create_graph, PGNN, NeuralNetCondMG\n",
    "from models.ours.Gaussians import MultivariateGaussain\n",
    "\n",
    "############################################################\n",
    "np.random.seed(7)\n",
    "\n",
    "# Gloable variables\n",
    "G_value_scale = 0.2          # the sscale we mess up the test set\n",
    "TRAINING_RATIO = 0.85\n",
    "VALID_RATIO = 0.2\n",
    "PARENT_RATIO = 0.3           # control the number of parents in the GBN template\n",
    "CORR_THRESH = 0.05           # similar function as above\n",
    "EPS = 1e-2\n",
    "VERBOSE = 1\n",
    "# px_learning_step = mg_learning_step\n",
    "px_learning_step = partial(mixmg_learning_step, n_comp = 3)\n",
    "\n",
    "############################################################\n",
    "\n",
    "# all dataset meta-data specify\n",
    "default_options = MyObject()\n",
    "default_options.root_dir = '/home/leondong/proj/robust/dataset/'\n",
    "default_options.down_sample = False\n",
    "default_options.with_label = False\n",
    "default_options.num_per_class = 1000\n",
    "default_options.normalize = True\n",
    "default_options.transform = None\n",
    "\n",
    "dataset_names = [('mnist', 'mnist/'), ('airquality','airquality/AirQualityUCI.csv'), \n",
    "                 ('parkinson', 'parkinson/parkinsons_updrs.data'),\n",
    "                ('energy','energy/data.csv'), ('hepmass','hepmass/hepmass.h5'),\n",
    "                ('miniboone', 'miniboone/miniboone.h5'), \n",
    "                 ('onlinenews','onlinenews/OnlineNewsPopularity.csv'),\n",
    "                ('superconduct','superconduct/data.csv'),\n",
    "                ('sdd', 'SDD/Sensorless_drive_diagnosis.txt')]\n",
    "\n",
    "loader_options = MyObject()\n",
    "for name, path in dataset_names: \n",
    "    l_op = deepcopy(default_options)\n",
    "    l_op.data_path = path\n",
    "    loader_options[name] = l_op\n",
    "\n",
    "# custom field for some dataset\n",
    "loader_options.mnist.transform = './output/vae-mnist-e250-d20.pkl'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0314f1b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:59.061772Z",
     "iopub.status.busy": "2023-10-15T04:06:59.061577Z",
     "iopub.status.idle": "2023-10-15T04:06:59.191958Z",
     "shell.execute_reply": "2023-10-15T04:06:59.191253Z",
     "shell.execute_reply.started": "2023-10-15T04:06:59.061754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parkinson data .....\n",
      "parkinson load complete!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# load the given dataset\n",
    "cur_options = loader_options[data_name]\n",
    "loader_module = importlib.import_module('loaders.{}'.format(data_name))\n",
    "dataset = loader_module.load_data(cur_options)\n",
    "\n",
    "# train, test split\n",
    "if len(dataset) == 2:\n",
    "    train, test = dataset\n",
    "else:\n",
    "    np.random.shuffle(dataset)\n",
    "    n_train = int(dataset.shape[0] * TRAINING_RATIO)\n",
    "    train = dataset[: n_train]\n",
    "    test = dataset[n_train:]\n",
    "\n",
    "# convert to float32 type\n",
    "train = train.astype('f4')\n",
    "test = test.astype('f4')\n",
    "\n",
    "# handle test case \n",
    "if testing:\n",
    "    train = train[:1000]\n",
    "    test = test[:100]\n",
    "\n",
    "if cur_options.transform is None:\n",
    "    # do standardize\n",
    "    scaler = StandardScaler().fit(train)\n",
    "    train, test = [scaler.transform(x) for x in [train,test]]\n",
    "      \n",
    "# shuffle the train split\n",
    "assert(len(train.shape) == 2), \"Data size does not match\"\n",
    "np.random.shuffle(train)\n",
    "    \n",
    "# train, valid split\n",
    "n_valid = int(train.shape[0] * VALID_RATIO)\n",
    "valid = train[:n_valid]\n",
    "train = train[n_valid:]\n",
    "\n",
    "# random messup the test\n",
    "test_gaussian = gaussian_noise(test, 1, G_value_scale)\n",
    "test_pj = pixel_jitter(test, 0.2, -G_value_scale, G_value_scale)\n",
    "\n",
    "# conduct transformation \n",
    "if cur_options.transform is not None:\n",
    "    model = utils.pkload(cur_options.transform)\n",
    "    model.model.to('cpu')\n",
    "    model.device = 'cpu'\n",
    "    train,valid,test,test_gaussian,test_pj = [model.transform(data) \n",
    "                             for data in [train,valid,test,test_gaussian,test_pj]]\n",
    "    \n",
    "\n",
    "# wrap up the dataset together\n",
    "dataset = MyObject()\n",
    "dataset.train = train\n",
    "dataset.valid = valid\n",
    "# dataset.valid = np.vstack([valid, gaussian_noise(valid, 1, G_value_scale)])\n",
    "dataset.test = [test, test_gaussian, test_pj]\n",
    "\n",
    "logger.write(f'{data_name} load complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b1f92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:59.193276Z",
     "iopub.status.busy": "2023-10-15T04:06:59.193077Z",
     "iopub.status.idle": "2023-10-15T04:06:59.203205Z",
     "shell.execute_reply": "2023-10-15T04:06:59.202496Z",
     "shell.execute_reply.started": "2023-10-15T04:06:59.193258Z"
    }
   },
   "outputs": [],
   "source": [
    "# find cutset variables using heuristical algorithm\n",
    "COND_RATIO = 0.25\n",
    "MIN_COND_NUM = 1\n",
    "xid = list( variance_methods( dataset.train , (COND_RATIO, MIN_COND_NUM) ) )\n",
    "yid = list( np.setdiff1d( np.arange(dataset.train.shape[1]), xid) ) \n",
    "\n",
    "############################################################\n",
    "dim_for_95var = pca_analysis(dataset.train, percentiles = [0.95])[0]\n",
    "hyper_parameters =  None #  (0.01, 1e-4) \n",
    "############################################################\n",
    "pgnn_conf = MyObject()\n",
    "pgnn_conf.depth = 3\n",
    "pgnn_conf.drop_out = 0.0\n",
    "pgnn_conf.compress_rate = 2\n",
    "pgnn_conf.prec_thresh = (1e-2, 1e+2)\n",
    "pgnn_conf.feature_size = dim_for_95var * int( 1+np.log(dataset.train.shape[1]) )\n",
    "pgnn_conf.max_header_size = len(yid)\n",
    "pgnn_conf.xid = xid\n",
    "pgnn_conf.yid = yid\n",
    "\n",
    "train_conf = MyObject()\n",
    "train_conf.init_epoch = 50\n",
    "train_conf.n_step = 150\n",
    "train_conf.n_epoch = 1\n",
    "# train_conf.batch_size = 512\n",
    "train_conf.adversial = True\n",
    "train_conf.delta = float(G_delta)\n",
    "train_conf.n_pretrain = 4\n",
    "train_conf.pre_epochs = 25\n",
    "train_conf.model_objs = None\n",
    "\n",
    "# adaptive batch size \n",
    "min_batch_size = 64\n",
    "min_iter_per_epoch = 100\n",
    "n_train_nums = dataset.train.shape[0]\n",
    "expect_bs_size = int(n_train_nums / min_iter_per_epoch)\n",
    "train_conf.batch_size = max(expect_bs_size, min_batch_size)\n",
    "\n",
    "if testing:\n",
    "    train_conf.init_epoch = 25\n",
    "    train_conf.n_step = 25\n",
    "    train_conf.pre_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7be1b9-143b-41ef-8049-095723d110ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:06:59.206376Z",
     "iopub.status.busy": "2023-10-15T04:06:59.206018Z",
     "iopub.status.idle": "2023-10-15T04:07:40.914441Z",
     "shell.execute_reply": "2023-10-15T04:07:40.913713Z",
     "shell.execute_reply.started": "2023-10-15T04:06:59.206354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_epoch -> 25\n",
      "    n_step -> 25\n",
      "   n_epoch -> 1\n",
      " adversial -> True\n",
      "     delta -> 0.5\n",
      "n_pretrain -> 4\n",
      "pre_epochs -> 10\n",
      "batch_size -> 64\n",
      "          depth -> 3\n",
      "       drop_out -> 0.0\n",
      "  compress_rate -> 2\n",
      "    prec_thresh -> (0.01, 100.0)\n",
      "   feature_size -> 21\n",
      "max_header_size -> 11\n",
      "            xid -> [11, 13, 7, 14]\n",
      "            yid -> [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12]\n",
      "Step 0, Learning epoch 0, avg loss: 5.55369297027587917964\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25508/717268461.py:10: RuntimeWarning: overflow encountered in power\n",
      "  return np.log(np.power(w,w)).sum() <= M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Learning epoch 0, avg loss: 3.9758269691467287283\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25508/717268461.py:10: RuntimeWarning: overflow encountered in power\n",
      "  return np.log(np.power(w,w)).sum() <= M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper parameter is: (0.01, 0.0001)2224799442291263\n",
      "TestLL p25:-10.4361 median:-6.7038 p75:-4.3833 Mean:-9.9534 Std:17.0556\n",
      "TestLL p25:-14.9713 median:-11.4783 p75:-8.6374 Mean:-14.353 Std:16.4938\n",
      "TestLL p25:-19.7064 median:-11.1181 p75:-7.7839 Mean:-17.7732 Std:23.3438\n",
      "Worst NB LL p25:-13.6257 median:-9.1123 p75:-6.9068 Mean:-12.9177 Std:18.2826\n",
      "Avg NB LL p25:-10.7718 median:-7.0435 p75:-4.8906 Mean:-10.3332 Std:17.0049\n"
     ]
    }
   ],
   "source": [
    "# run adversial experiment\n",
    "cuda_device = 'cuda:{}'.format(os.getpid() % 2)\n",
    "cnet_adv = run_exp(dataset, train_conf, pgnn_conf, param = hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78aad96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:07:40.915581Z",
     "iopub.status.busy": "2023-10-15T04:07:40.915329Z",
     "iopub.status.idle": "2023-10-15T04:08:19.900059Z",
     "shell.execute_reply": "2023-10-15T04:08:19.899359Z",
     "shell.execute_reply.started": "2023-10-15T04:07:40.915558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_epoch -> 25\n",
      "    n_step -> 25\n",
      "   n_epoch -> 1\n",
      " adversial -> False\n",
      "     delta -> 0.5\n",
      "n_pretrain -> 4\n",
      "pre_epochs -> 10\n",
      "batch_size -> 64\n",
      "          depth -> 3\n",
      "       drop_out -> 0.0\n",
      "  compress_rate -> 2\n",
      "    prec_thresh -> (0.01, 100.0)\n",
      "   feature_size -> 21\n",
      "max_header_size -> 11\n",
      "            xid -> [11, 13, 7, 14]\n",
      "            yid -> [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12]\n",
      "Best hyper parameter is: (0.01, 0.0001)2324804592132574834\n",
      "TestLL p25:-10.1441 median:-6.3774 p75:-3.9273 Mean:-9.4768 Std:16.2435\n",
      "TestLL p25:-14.7325 median:-11.5488 p75:-8.5247 Mean:-13.8896 Std:15.1321\n",
      "TestLL p25:-18.3973 median:-11.0145 p75:-8.0415 Mean:-17.1029 Std:19.1755\n",
      "Worst NB LL p25:-13.0466 median:-9.0963 p75:-6.3424 Mean:-12.4194 Std:17.3611\n",
      "Avg NB LL p25:-10.462 median:-6.7142 p75:-4.5604 Mean:-9.8554 Std:16.2051\n"
     ]
    }
   ],
   "source": [
    "# non-adversial case\n",
    "train_conf.adversial = False\n",
    "# train_conf.n_step, train_conf.n_epoch = train_conf.n_epoch, train_conf.n_step\n",
    "cnet_std = run_exp(dataset, train_conf, pgnn_conf, param = hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936df9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6813f55-9707-4266-a07c-77ea70a3f628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d85c9-0bc8-43be-8c30-2bc58597c275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6782f3e2-9009-44df-ad4d-6957f0daf698",
   "metadata": {},
   "source": [
    "## The following code is used for analysis purpose only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e55fbe15-79fb-4b2c-8e60-804295ac500d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:08:19.901064Z",
     "iopub.status.busy": "2023-10-15T04:08:19.900862Z",
     "iopub.status.idle": "2023-10-15T04:08:19.909800Z",
     "shell.execute_reply": "2023-10-15T04:08:19.909192Z",
     "shell.execute_reply.started": "2023-10-15T04:08:19.901045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.953390432984197"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana_log = Logger(log_file + '.ana', with_time = False)\n",
    "ori, gau, pj = dataset.test\n",
    "cnet_adv.mass(ori,logmode= True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48f22891-a0a2-44e6-8d93-d2fab97d5700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:08:19.910618Z",
     "iopub.status.busy": "2023-10-15T04:08:19.910447Z",
     "iopub.status.idle": "2023-10-15T04:08:20.002321Z",
     "shell.execute_reply": "2023-10-15T04:08:20.001019Z",
     "shell.execute_reply.started": "2023-10-15T04:08:19.910602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.47678106525377"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnet_std.mass(ori,logmode= True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebc45bd7-68c6-41c4-82aa-5ba7efe033e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:08:20.004745Z",
     "iopub.status.busy": "2023-10-15T04:08:20.004087Z",
     "iopub.status.idle": "2023-10-15T04:08:20.095472Z",
     "shell.execute_reply": "2023-10-15T04:08:20.094049Z",
     "shell.execute_reply.started": "2023-10-15T04:08:20.004696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.953390432984197\n",
      "([-6.1147, -3.8103, -3.0088, -4.6572, 2.2846], [-4.6021, -2.4867, -0.6518, -5.2962, 15.9592])\n"
     ]
    }
   ],
   "source": [
    "def mass_ex(self, Z, logmode = True):\n",
    "    # compute the density of samples, can be single sample as well\n",
    "    if len(Z.shape) == 1:\n",
    "        Z = Z.reshape(1, -1)\n",
    "    \n",
    "    X = Z[:, self.xids]\n",
    "    Y = Z[:, self.yids]\n",
    "    \n",
    "    massX = self.px.mass(X, logmode = True)\n",
    "    massY = self.py.mass(Y, X, logmode = True)\n",
    "    \n",
    "    return [massX, massY]\n",
    "\n",
    "mX,mY = mass_ex(cnet_adv, ori)\n",
    "print(np.mean(mX + mY))\n",
    "ana_log.write( (five_number_statistic(mX), five_number_statistic(mY)), echo = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3427ea19-0af3-4526-836a-15a30125facb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T04:08:20.098144Z",
     "iopub.status.busy": "2023-10-15T04:08:20.097269Z",
     "iopub.status.idle": "2023-10-15T04:08:20.240344Z",
     "shell.execute_reply": "2023-10-15T04:08:20.238924Z",
     "shell.execute_reply.started": "2023-10-15T04:08:20.098094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-5.4827, -3.6317, -2.8554, -4.4188, 2.5392], [-4.7518, -2.5521, -0.7041, -5.058, 14.2557])\n"
     ]
    }
   ],
   "source": [
    "mX,mY = mass_ex(cnet_std, ori)\n",
    "ana_log.write( (five_number_statistic(mX), five_number_statistic(mY)), echo = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c098c-94db-4a55-87a2-6c167cde8880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6e784-231b-4e28-8bf3-0172b6716390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d214c7-f489-4178-9adf-192b132aa61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
