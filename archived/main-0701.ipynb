{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os, pdb, warnings\n",
    "sys.path.insert(0, './core/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from minilib import *\n",
    "from utmLib import utils\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.rcParams[\"figure.figsize\"] = [10,2]\n",
    "\n",
    "testing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_imgs(img_array, rows = 2, cols = 8, selected = None, label_array = None):\n",
    "    if img_array[0].size == 196:\n",
    "        process = lambda x:x.reshape(14,14)\n",
    "    else:\n",
    "        process = lambda x:x.reshape(28,28)\n",
    "    \n",
    "    # random select some examples for display if not specified\n",
    "    if selected is None:\n",
    "        assert(img_array.shape[0] >= rows * cols)\n",
    "        selected = np.random.choice(img_array.shape[0], rows * cols, replace = False)\n",
    "    else:\n",
    "        assert(selected.size >= rows * cols)\n",
    "    \n",
    "    k = 1\n",
    "    labels = []\n",
    "    fid = plt.figure()\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            plt.subplot(rows, cols, k)\n",
    "            plt.imshow(process(img_array[selected[k-1]]), cmap='gray' )\n",
    "            if label_array is not None:\n",
    "                labels.append(label_array[selected[k-1]])\n",
    "            k += 1\n",
    "            plt.axis('off')\n",
    "    \n",
    "    if len(labels):\n",
    "        print(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b70dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversial_step(masses, delta = 0.5):\n",
    "    \n",
    "    def get_w(alpha):\n",
    "        w = np.exp( -masses / alpha )\n",
    "        w = w / np.sum(w) * N\n",
    "        return w\n",
    "    \n",
    "    def valid(alpha):\n",
    "        w = get_w(alpha)\n",
    "        return np.log(np.power(w,w)).sum() <= M\n",
    "    \n",
    "    # use the idea of binary search, time complexity O(N * lgN)\n",
    "    N = masses.size\n",
    "    M = N * delta\n",
    "    l = 0.01\n",
    "    r = 2 ** 10\n",
    "    \n",
    "    # need to gurantee that r is big enough \n",
    "    while not valid(r):\n",
    "        r = r * 2\n",
    "    \n",
    "    # binary search a valid alpha in range [l,r] \n",
    "    while r - l > 1e-2:\n",
    "        m = (l+r) / 2\n",
    "\n",
    "        if valid(m):\n",
    "            r = m \n",
    "        else:\n",
    "            l = m\n",
    "    \n",
    "    return get_w(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_learning_step(step, R, data_loader, model, optimizer, scheduler, max_epoch):\n",
    "    model.train()\n",
    "    R = R.astype('f4').reshape(-1, 1)\n",
    "    R_tensor = torch.from_numpy( R ).to(model.device)\n",
    "    \n",
    "    for e in range(max_epoch):\n",
    "        acc_loss = 0\n",
    "        total = 0\n",
    "\n",
    "        for X, Y, _i in data_loader:\n",
    "            X = X.to(model.device)\n",
    "            Y = Y.to(model.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(X)\n",
    "            loss = model.loss(out, Y, R = R_tensor[_i])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss += loss.item()\n",
    "            total += X.shape[0]\n",
    "        \n",
    "        # step is global variable\n",
    "        scheduler.step()\n",
    "        if VERBOSE:\n",
    "            print(f'Step {step}, Learning epoch {e}, avg loss: {acc_loss/total}', end = '\\r')\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ours.Gaussians import MultivariateGaussain\n",
    "\n",
    "def mg_learning_step(R, data, **kwargs):\n",
    "    # learn a gaussian distribution from weighted data\n",
    "    N, D = data.shape \n",
    "    mg = MultivariateGaussain()\n",
    "    R = R.reshape(N, 1)\n",
    "    mg.mu = np.mean( R * data, axis = 0)\n",
    "    \n",
    "    mat = data - mg.mu.reshape(1, D)\n",
    "    mat2 = mat.copy()\n",
    "    mat2 = mat2 * R / N\n",
    "    S2 = mat.T @ mat2\n",
    "    \n",
    "    mg.S = S2\n",
    "    return mg\n",
    "\n",
    "def mixmg_learning_step(R, data, n_comp, **kwargs):\n",
    "    step = kwargs['step']\n",
    "    total = kwargs['total']\n",
    "    adv = kwargs['adv']\n",
    "    wrapper = kwargs['wrapper']\n",
    "    \n",
    "    if wrapper[0] is None:\n",
    "        wrapper[0] = MixMGLearner(max_iter = 0, n_components = n_comp, reg_covar = 1e-4).fit(data)\n",
    "    \n",
    "    mixmg_object = wrapper[0]\n",
    "    \n",
    "    if not adv:\n",
    "        niter = total    \n",
    "    elif step == 0 or step == total-1 :\n",
    "        niter = 16\n",
    "    else:\n",
    "        niter = 4\n",
    "        \n",
    "    for _ in range(niter):\n",
    "        mixmg_object._estep()\n",
    "        mixmg_object._mstep(data, R)\n",
    "    \n",
    "    return mixmg_object.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46083462",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load all digits datasets\n",
    "import numpy as np\n",
    "import loader\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "mnist_dir = './data/digits/mnist'\n",
    "ch74_dir = './data/digits/chars74k'\n",
    "dida_dir = './data/digits/dida'\n",
    "NUM_PER_CLASS = 50\n",
    "DOWN_SAMPLE = False\n",
    "\n",
    "# fix seed for fixed split \n",
    "np.random.seed(3)\n",
    "\n",
    "# load all dataset and convert into certain format\n",
    "(mnist_train, mnist_train_label), (mnist_test, mnist_test_label) = loader.read_mnist(mnist_dir, \\\n",
    "                                                                     down_sample = DOWN_SAMPLE, with_label = True)\n",
    "\n",
    "ch74 = loader.read_chars74k(ch74_dir, NUM_PER_CLASS, with_label = True)\n",
    "ch74_imgs, ch74_labels = list(zip(*ch74))\n",
    "ch74 = np.array(list(map(partial(loader.transform_to_mnist, down_sample = DOWN_SAMPLE, normalize = False) ,ch74_imgs)))\n",
    "ch74_labels = np.array(ch74_labels, dtype = 'i4')\n",
    "\n",
    "dida = loader.read_dida(dida_dir, 1000, with_label = True)\n",
    "dida_imgs, dida_labels = list(zip(*dida))\n",
    "dida = np.array(list(map(partial(loader.transform_to_mnist, down_sample = DOWN_SAMPLE, normalize = True) ,dida_imgs)))\n",
    "dida_labels = np.array(dida_labels, dtype = 'i4')\n",
    "\n",
    "# augment mnist dataset for both black and white background\n",
    "mnist_train = np.vstack([mnist_train, 1-mnist_train])\n",
    "mnist_test =  np.vstack([mnist_test, 1-mnist_test])\n",
    "mnist_train_label = np.hstack([mnist_train_label, mnist_train_label])\n",
    "mnist_test_label = np.hstack([mnist_test_label, mnist_test_label])\n",
    "\n",
    "# gather all dataset together in one dictionary\n",
    "all_datasets = defaultdict(dict)\n",
    "all_datasets['mnist']['train'] = (mnist_train, mnist_train_label)\n",
    "all_datasets['mnist']['test'] = (mnist_test, mnist_test_label)\n",
    "all_datasets['dida']['all'] = (dida, dida_labels)\n",
    "all_datasets['ch74']['all'] = (ch74, ch74_labels)\n",
    "\n",
    "# shuffle all datasets along with its label\n",
    "for ds_name in all_datasets:\n",
    "    current_splits = list(all_datasets[ds_name].keys())\n",
    "    for split in current_splits:\n",
    "        data, label = all_datasets[ds_name][split]\n",
    "        label = label.astype('i8')\n",
    "        inds = np.arange(data.shape[0])\n",
    "        np.random.shuffle(inds)\n",
    "        all_datasets[ds_name][split] = (data[inds], label[inds])\n",
    "        \n",
    "        if split == 'train':\n",
    "            # split into train and val\n",
    "            train_ratio = 0.8\n",
    "            names = ['train', 'val']\n",
    "        elif split == 'all':\n",
    "            # split into train and test for dida and ch74\n",
    "            train_ratio = 0.5\n",
    "            names = ['train', 'test']\n",
    "        else:\n",
    "            train_ratio = 0\n",
    "        \n",
    "        if train_ratio == 0:\n",
    "            continue\n",
    "        \n",
    "        data, label = all_datasets[ds_name][split]\n",
    "        size = int(data.shape[0] * train_ratio)\n",
    "        na,nb = names\n",
    "        all_datasets[ds_name][na] = (data[:size], label[:size])\n",
    "        all_datasets[ds_name][nb] = (data[size:], label[size:])\n",
    "\n",
    "\n",
    "# do some jittering to the pixels of mnist_test\n",
    "mnist_adv_gaussain = deepcopy(all_datasets['mnist']['test'][0])\n",
    "mnist_adv_gaussain += np.clip(np.random.normal(size = mnist_adv_gaussain.shape, scale = 0.5), -0.2, 0.2)\n",
    "mnist_adv_gaussain = np.clip(mnist_adv_gaussain, 0, 1)\n",
    "all_datasets['mnist']['adv_gaussain'] = (mnist_adv_gaussain, all_datasets['mnist']['test'][1] )\n",
    "\n",
    "n_jitter_pixel = int(mnist_train.shape[1] * 0.1)\n",
    "mnist_adv_pixel =  deepcopy(all_datasets['mnist']['test'][0])\n",
    "for item in mnist_adv_pixel:\n",
    "    selected = np.random.choice(item.size, size = n_jitter_pixel, replace = False)\n",
    "    item[selected] = np.random.uniform( size = selected.size )\n",
    "all_datasets['mnist']['adv_pixel'] = (mnist_adv_pixel, all_datasets['mnist']['test'][1] )\n",
    "\n",
    "# visulize the datasets\n",
    "for ds_name in all_datasets:\n",
    "    for split in all_datasets[ds_name]:\n",
    "        dataset, labels = all_datasets[ds_name][split]\n",
    "        assert(dataset.shape[0] == labels.size)\n",
    "        print('The number of samples in {} is {}'.format(f'{ds_name} {split}' ,dataset.shape[0]))\n",
    "        assert(np.all(0 <= dataset) and np.all(dataset <= 1))\n",
    "        print(f\"Pixel value range is from {np.min(dataset)} to {np.max(dataset)}.\")\n",
    "        visualize_imgs(dataset, label_array = labels)\n",
    "        \n",
    "        # reduce size if testing \n",
    "        if testing:\n",
    "            all_datasets[ds_name][split] = (dataset[:1000], labels[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f88dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map into embeded space using auto encoder\n",
    "data_transform = utils.pkload('./output/cnn_auto_encoder.pkl')\n",
    "for name in all_datasets:\n",
    "    for split in all_datasets[name]:\n",
    "        X,y = all_datasets[name][split]\n",
    "        F = data_transform.transform(X)\n",
    "        all_datasets[name][split] = (F,y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, data, _x, _y):\n",
    "        self.X = data[:, _x]\n",
    "        self.Y = data[:, _y]\n",
    "        self.total = data.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.X[ind], self.Y[ind], ind\n",
    "\n",
    "def five_number_statistic(logmass):\n",
    "    p25, median, p75 = np.percentile(logmass, [25,50,75])\n",
    "    average = np.mean(logmass)\n",
    "    std = np.std(logmass)\n",
    "    ret = (p25, median, p75, average, std)\n",
    "    return list(np.round(ret, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function for image classification\n",
    "def evaluate(model, img_arr, device):\n",
    "    imgs = img_arr.reshape(-1, 1, 14, 14)\n",
    "    test = torch.from_numpy(imgs).to(device)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=1000, shuffle=False, drop_last=False)\n",
    "\n",
    "    ret = []\n",
    "    with torch.no_grad():\n",
    "        for X in test_loader:\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            ret.append(predicted)\n",
    "    ret = torch.cat(ret, dim = 0)\n",
    "    ret = ret.cpu().numpy()\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, train_conf, model_conf, lr, wd, device):\n",
    "    model_conf.device = device\n",
    "    xid = model_conf.xid\n",
    "    yid = model_conf.yid\n",
    "    num_parents = model_conf.num_parents\n",
    "    structure = model_conf.structure\n",
    "    \n",
    "    batch_size = train_conf.batch_size\n",
    "    init_epoch = train_conf.init_epoch\n",
    "    n_epoch = train_conf.n_epoch\n",
    "    n_step = train_conf.n_step\n",
    "    adv = train_conf.adversial\n",
    "    delta = train_conf.delta\n",
    "    \n",
    "    model = PGNN( len(xid) , num_parents, model_conf)\n",
    "    model.moveto(device)\n",
    "    model.loss = partial(model.weighted_loss_func, G = structure)\n",
    "    \n",
    "    # init\n",
    "    px_container = [None]  # for mixmg px part use only in concurrent environment\n",
    "    R = np.ones( shape = (train_data.shape[0], ) )\n",
    "    total_epochs = init_epoch + n_epoch + n_step -1 \n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay = wd )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, anneal_strategy='cos', pct_start=0.25,\n",
    "                            epochs= total_epochs, steps_per_epoch = 1, verbose = False)\n",
    "\n",
    "    py = NeuralNetCondMG()\n",
    "    py.nn = model\n",
    "    py.gbn = GBN(structure).fit(train_data[:, yid], var_thresh = 1e-2)\n",
    "\n",
    "    # init training of NN\n",
    "    data_loader = DataLoader( MyData(train_data, xid, yid) , batch_size = batch_size)\n",
    "    py_learning_step('Init', R, data_loader, model, optimizer, scheduler, max_epoch = init_epoch)\n",
    "\n",
    "    # begining of (adversial robust) training \n",
    "    for step in range(n_step):\n",
    "        # conduct learning step first\n",
    "        model.train()\n",
    "        mg = px_learning_step(R, train_data[:, xid], step = step, total = n_step,\n",
    "                              adv = adv, wrapper = px_container)\n",
    "        py_learning_step(step, R, data_loader, model, optimizer, scheduler, max_epoch = n_epoch)\n",
    "        model.eval()\n",
    "\n",
    "        # evaluate the mass of each data\n",
    "        cnet = ContCNet(mg, py, xid, yid)\n",
    "        masses = cnet.mass(train_data, logmode = True)\n",
    "\n",
    "        # conduct adversial step\n",
    "        R = adversial_step(masses, delta = delta)\n",
    "\n",
    "        # verify if R satisfy the constrains \n",
    "        sat1 = np.sum(R) <= ( train_data.shape[0] + EPS )\n",
    "        sat2 = np.sum( np.log( np.power(R, R) ) ) <= ( train_data.shape[0] * delta + EPS )\n",
    "        \n",
    "        if not (sat1 and sat2):\n",
    "            warnings.warn(f'Constrains not satisfied during adversial step.')\n",
    "            break   \n",
    "\n",
    "    model.eval()\n",
    "    model.moveto('cpu')\n",
    "    score = cnet.mass(val_data, logmode = True).mean()\n",
    "    \n",
    "    return cnet, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e695bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_exp(train_data, val_data, train_conf, nn_conf,\n",
    "            completion = False, param = None, \n",
    "            n_test = 200, n_rows = 2, n_cols = 10):\n",
    "    \n",
    "    \n",
    "    xid = nn_conf.xid\n",
    "    yid = nn_conf.yid\n",
    "    model_conf = deepcopy(nn_conf)\n",
    "    structure = create_graph(train_data[:, yid], \n",
    "                    max_parents = max(MIN_PARENT, int(PARENT_RATIO * train_data.shape[1])), \n",
    "                    corr_thresh = CORR_THRESH)\n",
    "\n",
    "    model_conf.num_parents = [len(structure.V[i].parents) for i in range(structure.N)]\n",
    "    model_conf.structure = structure\n",
    "\n",
    "    # do hyper selection if param is not specified\n",
    "    if param is None:\n",
    "        if testing:\n",
    "            learning_rates = [5e-3, 1e-2]\n",
    "            weight_decays = [1e-4]\n",
    "        else:\n",
    "            learning_rates = [1e-2, 3e-3, 1e-3]\n",
    "            weight_decays = [1e-4, 1e-5, 0]\n",
    "\n",
    "        all_params = list(product(learning_rates, weight_decays))\n",
    "\n",
    "        results = Parallel(n_jobs = 2, prefer = 'threads')(\n",
    "            delayed(train_model)(train_data, val_data, train_conf, model_conf,\n",
    "                    *comb, 'cuda:{}'.format(i%2)) \n",
    "            for i, comb in enumerate(all_params)\n",
    "        )\n",
    "\n",
    "        models, scores = list(zip(*results))\n",
    "        ind = np.argmax(scores)\n",
    "        cnet = models[ind]\n",
    "        print('Best hyper parameter is: {}'.format(all_params[ind]))\n",
    "    else:\n",
    "        cnet = train_model(train_data, val_data, train_conf, model_conf, *param, 'cuda:0')[0]\n",
    "\n",
    "    # conduct LL test if not completion task \n",
    "    if not completion:\n",
    "        # evaluate the loglikelihood \n",
    "        for ds_name in all_datasets:\n",
    "            for split in all_datasets[ds_name]:\n",
    "                if not ('adv' in split or 'test' in split):\n",
    "                    continue\n",
    "                dataset, labels = all_datasets[ds_name][split]\n",
    "                \n",
    "                cur_mass = cnet.mass(dataset, logmode = True)\n",
    "                print('-'*100)\n",
    "                print(f'LL on {ds_name} {split}:')\n",
    "                print('p25:{} median:{} p75:{} Mean:{} Std:{}'.format(*five_number_statistic(cur_mass)))\n",
    "                print('-'*100)\n",
    "        return\n",
    "\n",
    "    \n",
    "    # conduct image completion test, assume it is always given \n",
    "    plt.rcParams[\"figure.figsize\"] = [ncols, nrows]\n",
    "    mnist_test = all_datasets['mnist']['test'][0]\n",
    "    \n",
    "    for ds_name in all_datasets:\n",
    "        for split in all_datasets[ds_name]:\n",
    "            if not ('adv' in split or 'test' in split):\n",
    "                continue\n",
    "\n",
    "            dataset, labels = all_datasets[ds_name][split]\n",
    "            name = ds_name + ' ' + split\n",
    "\n",
    "            print('-'*100)\n",
    "            print(f'Completion on {name}')\n",
    "            np.random.seed(7)\n",
    "            selected = np.random.choice(dataset.shape[0], n_test, replace = False)\n",
    "            preds = []\n",
    "            rmses = []\n",
    "            for i, item in enumerate(dataset[selected]):\n",
    "                if 'adv_' in name:\n",
    "                    gt = mnist_test[selected[i]][cnet.yids]\n",
    "                else:\n",
    "                    gt = item[cnet.yids]\n",
    "                item[cnet.yids] = np.nan\n",
    "                img = cnet.map_via_cutset_sampling(item, cnet.yids)\n",
    "                # use gradient ascent to optimize the assignment for restricted domain \n",
    "                img = cnet.optimize_assignment(img, cnet.yids)\n",
    "                img = np.clip(img, 0, 1)\n",
    "                preds.append( img )\n",
    "                diff = img[cnet.yids] - gt\n",
    "                rmses.append( np.sqrt( np.mean( np.square(diff) )) )\n",
    "\n",
    "            print('RMSE - p25:{} median:{} p75:{} Mean:{} Std:{}'.format(*five_number_statistic(rmses)))\n",
    "            pred_labels = evaluate(img_classifier, np.array(preds))\n",
    "            acc = np.sum(pred_labels == labels[selected]) / pred_labels.size\n",
    "            print('On {} {} image complection, accuracy is {}'.format(ds_name, split, acc))\n",
    "\n",
    "            visualize_imgs(preds, nrows, ncols, np.arange(N_TEST))\n",
    "            print('-' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4a491",
   "metadata": {},
   "source": [
    "# Experiement of Loglikelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some common parameters across experiments\n",
    "import torch\n",
    "from models.ours.ContCNet import ContCNet\n",
    "from utmLib.ml.GBN import GBN\n",
    "from models.ours.NNCondMG import MyObject, create_graph, PGNN, NeuralNetCondMG\n",
    "from utmLib import utils\n",
    "\n",
    "############################################################\n",
    "# this is global variables \n",
    "PARENT_RATIO = 0.3\n",
    "CORR_THRESH = 0.09\n",
    "MIN_PARENT = 3\n",
    "EPS = 1e-2\n",
    "VERBOSE = True\n",
    "# px_learning_step = mg_learning_step\n",
    "px_learning_step = partial(mixmg_learning_step, n_comp = 3)\n",
    "############################################################\n",
    "# img_classifier = utils.pkload('output/lenet5.pkl')\n",
    "# img_classifier.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1f92a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = all_datasets['mnist']['train'][0]\n",
    "val_data = all_datasets['mnist']['val'][0]\n",
    "\n",
    "# find cutset variables using heuristical algorithm\n",
    "COND_RATIO = 0.3\n",
    "MIN_COND_NUM = 5\n",
    "xid = list( variance_methods( train_data , (COND_RATIO, MIN_COND_NUM) ) )\n",
    "yid = list( np.setdiff1d( np.arange(train_data.shape[1]), xid) ) \n",
    "\n",
    "############################################################\n",
    "pgnn_conf = MyObject()\n",
    "pgnn_conf.depth = 3\n",
    "pgnn_conf.drop_out = 0.0\n",
    "pgnn_conf.compress_rate = 2\n",
    "pgnn_conf.prec_thresh = (1e-2, 1e+2)\n",
    "pgnn_conf.feature_size = len(yid)\n",
    "pgnn_conf.max_header_size = len(yid)\n",
    "pgnn_conf.xid = xid\n",
    "pgnn_conf.yid = yid\n",
    "\n",
    "train_conf = MyObject()\n",
    "train_conf.init_epoch = 50\n",
    "train_conf.n_step = 100\n",
    "train_conf.n_epoch = 1\n",
    "train_conf.batch_size = 512\n",
    "train_conf.adversial = True\n",
    "train_conf.delta = 1.0\n",
    "\n",
    "# run experiment\n",
    "run_exp(train_data, val_data, train_conf, pgnn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aad96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-adversial case\n",
    "train_conf.adversial = False\n",
    "train_conf.n_step, train_conf.n_epoch = train_conf.n_epoch, train_conf.n_step\n",
    "run_exp(train_data, val_data, train_conf, pgnn_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936df9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beea640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e7773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
